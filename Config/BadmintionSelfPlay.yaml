behaviors:
  BadmintonSelfPlay:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048
      buffer_size: 20480
      learning_rate: 0.00025
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
    network_settings:
      normalize: true
      hidden_units: 256
      num_layers: 3
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
    self_play:
      save_steps: 50000                # how often to snapshot the current policy
      team_change: 200000              # steps before switching which team trains
      swap_steps: 2000                 # how often to swap opponent snapshots
      window: 10                       # how many past opponents to keep
      play_against_latest_model_ratio: 0.5   # 50% of matches use the latest policy
      initial_elo: 1200                # starting ELO rating
    max_steps: 50000000
    time_horizon: 128
    summary_freq: 50000

engine_settings:
  time_scale: 5   # speed up simulation
