{
    "name": "root",
    "gauges": {
        "Runner.Policy.Entropy.mean": {
            "value": 1.1940382719039917,
            "min": 1.1761876344680786,
            "max": 1.4243495464324951,
            "count": 486
        },
        "Runner.Policy.Entropy.sum": {
            "value": 865.677734375,
            "min": 353.9908447265625,
            "max": 2270.301513671875,
            "count": 486
        },
        "Runner.Step.mean": {
            "value": 486994.0,
            "min": 960.0,
            "max": 486994.0,
            "count": 487
        },
        "Runner.Step.sum": {
            "value": 486994.0,
            "min": 960.0,
            "max": 486994.0,
            "count": 487
        },
        "Runner.Policy.ExtrinsicValueEstimate.mean": {
            "value": -7.399129867553711,
            "min": -10.040006637573242,
            "max": 2.7963972091674805,
            "count": 487
        },
        "Runner.Policy.ExtrinsicValueEstimate.sum": {
            "value": -140.58346557617188,
            "min": -1172.133056640625,
            "max": 41.94595718383789,
            "count": 487
        },
        "Runner.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 487
        },
        "Runner.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 487
        },
        "Runner.Losses.PolicyLoss.mean": {
            "value": 0.09029191009190696,
            "min": 0.05741703610207575,
            "max": 0.1665917350063947,
            "count": 229
        },
        "Runner.Losses.PolicyLoss.sum": {
            "value": 0.09029191009190696,
            "min": 0.05741703610207575,
            "max": 0.1665917350063947,
            "count": 229
        },
        "Runner.Losses.ValueLoss.mean": {
            "value": 0.09097251994863459,
            "min": 2.061811666827256e-05,
            "max": 1.6920522445380086,
            "count": 229
        },
        "Runner.Losses.ValueLoss.sum": {
            "value": 0.09097251994863459,
            "min": 2.061811666827256e-05,
            "max": 1.6920522445380086,
            "count": 229
        },
        "Runner.Policy.LearningRate.mean": {
            "value": 0.0002708685097105,
            "min": 0.0002708685097105,
            "max": 0.000299808000064,
            "count": 229
        },
        "Runner.Policy.LearningRate.sum": {
            "value": 0.0002708685097105,
            "min": 0.0002708685097105,
            "max": 0.000299808000064,
            "count": 229
        },
        "Runner.Policy.Epsilon.mean": {
            "value": 0.19028949999999992,
            "min": 0.19028949999999992,
            "max": 0.19993599999999997,
            "count": 229
        },
        "Runner.Policy.Epsilon.sum": {
            "value": 0.19028949999999992,
            "min": 0.19028949999999992,
            "max": 0.19993599999999997,
            "count": 229
        },
        "Runner.Policy.Beta.mean": {
            "value": 0.004515446049999998,
            "min": 0.004515446049999998,
            "max": 0.0049968064000000005,
            "count": 229
        },
        "Runner.Policy.Beta.sum": {
            "value": 0.004515446049999998,
            "min": 0.004515446049999998,
            "max": 0.0049968064000000005,
            "count": 229
        },
        "Runner.Environment.EpisodeLength.mean": {
            "value": 11.5,
            "min": 3.0,
            "max": 12091.0,
            "count": 176
        },
        "Runner.Environment.EpisodeLength.sum": {
            "value": 46.0,
            "min": 8.0,
            "max": 12091.0,
            "count": 176
        },
        "Runner.Environment.CumulativeReward.mean": {
            "value": -9.987849950790405,
            "min": -851.3049221038818,
            "max": -7.736401312053204,
            "count": 176
        },
        "Runner.Environment.CumulativeReward.sum": {
            "value": -39.95139980316162,
            "min": -1299.4053688049316,
            "max": -7.736401312053204,
            "count": 176
        },
        "Runner.Policy.ExtrinsicReward.mean": {
            "value": -9.987849950790405,
            "min": -851.3049221038818,
            "max": -7.736401312053204,
            "count": 176
        },
        "Runner.Policy.ExtrinsicReward.sum": {
            "value": -39.95139980316162,
            "min": -1299.4053688049316,
            "max": -7.736401312053204,
            "count": 176
        },
        "Tagger.Policy.Entropy.mean": {
            "value": 1.3672226667404175,
            "min": 1.3672226667404175,
            "max": 1.4201228618621826,
            "count": 19
        },
        "Tagger.Policy.Entropy.sum": {
            "value": 1339.878173828125,
            "min": 1337.8292236328125,
            "max": 1460.08740234375,
            "count": 19
        },
        "Tagger.Step.mean": {
            "value": 18974.0,
            "min": 977.0,
            "max": 18974.0,
            "count": 19
        },
        "Tagger.Step.sum": {
            "value": 18974.0,
            "min": 977.0,
            "max": 18974.0,
            "count": 19
        },
        "Tagger.Policy.ExtrinsicValueEstimate.mean": {
            "value": -1.3560197353363037,
            "min": -5.606346130371094,
            "max": 0.1406046599149704,
            "count": 19
        },
        "Tagger.Policy.ExtrinsicValueEstimate.sum": {
            "value": -21.69631576538086,
            "min": -723.2186279296875,
            "max": 3.79632568359375,
            "count": 19
        },
        "Tagger.Environment.EpisodeLength.mean": {
            "value": 211.0,
            "min": 6.682170542635659,
            "max": 860.0,
            "count": 19
        },
        "Tagger.Environment.EpisodeLength.sum": {
            "value": 211.0,
            "min": 211.0,
            "max": 1590.0,
            "count": 19
        },
        "Tagger.Environment.CumulativeReward.mean": {
            "value": -4.992481424938887,
            "min": -5.954543039513131,
            "max": -4.992481424938887,
            "count": 19
        },
        "Tagger.Environment.CumulativeReward.sum": {
            "value": -4.992481424938887,
            "min": -645.0617990493774,
            "max": -4.992481424938887,
            "count": 19
        },
        "Tagger.Policy.ExtrinsicReward.mean": {
            "value": -4.992481424938887,
            "min": -5.954543039513131,
            "max": -4.992481424938887,
            "count": 19
        },
        "Tagger.Policy.ExtrinsicReward.sum": {
            "value": -4.992481424938887,
            "min": -645.0617990493774,
            "max": -4.992481424938887,
            "count": 19
        },
        "Tagger.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 19
        },
        "Tagger.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 19
        },
        "Tagger.Losses.PolicyLoss.mean": {
            "value": 0.09791625154806145,
            "min": 0.09032753322268643,
            "max": 0.11097834720082271,
            "count": 9
        },
        "Tagger.Losses.PolicyLoss.sum": {
            "value": 0.09791625154806145,
            "min": 0.09032753322268643,
            "max": 0.11097834720082271,
            "count": 9
        },
        "Tagger.Losses.ValueLoss.mean": {
            "value": 0.25657486329631257,
            "min": 0.05891608660264561,
            "max": 2.2492430731654167,
            "count": 9
        },
        "Tagger.Losses.ValueLoss.sum": {
            "value": 0.25657486329631257,
            "min": 0.05891608660264561,
            "max": 2.2492430731654167,
            "count": 9
        },
        "Tagger.Policy.LearningRate.mean": {
            "value": 0.00029888076037308,
            "min": 0.00029888076037308,
            "max": 0.00029987664004112,
            "count": 9
        },
        "Tagger.Policy.LearningRate.sum": {
            "value": 0.00029888076037308,
            "min": 0.00029888076037308,
            "max": 0.00029987664004112,
            "count": 9
        },
        "Tagger.Policy.Epsilon.mean": {
            "value": 0.19962692,
            "min": 0.19962692,
            "max": 0.19995888000000003,
            "count": 9
        },
        "Tagger.Policy.Epsilon.sum": {
            "value": 0.19962692,
            "min": 0.19962692,
            "max": 0.19995888000000003,
            "count": 9
        },
        "Tagger.Policy.Beta.mean": {
            "value": 0.004981383308,
            "min": 0.004981383308,
            "max": 0.004997948112000001,
            "count": 9
        },
        "Tagger.Policy.Beta.sum": {
            "value": 0.004981383308,
            "min": 0.004981383308,
            "max": 0.004997948112000001,
            "count": 9
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1758507800",
        "python_version": "3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\Kaylen\\OneDrive\\Desktop\\Unity Projects\\AI_GameProject_FYP\\venv\\Scripts\\mlagents-learn config/tagnrun.yaml --run-id=runner_vs_tagger --force",
        "mlagents_version": "1.1.0",
        "mlagents_envs_version": "1.1.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.8.0+cpu",
        "numpy_version": "1.23.5",
        "end_time_seconds": "1758508449"
    },
    "total": 649.6612537000037,
    "count": 1,
    "self": 0.009554700023727491,
    "children": {
        "run_training.setup": {
            "total": 0.1292220999894198,
            "count": 1,
            "self": 0.1292220999894198
        },
        "TrainerController.start_learning": {
            "total": 649.5224768999906,
            "count": 1,
            "self": 0.5260220000636764,
            "children": {
                "TrainerController._reset_env": {
                    "total": 64.85038780001923,
                    "count": 1,
                    "self": 64.85038780001923
                },
                "TrainerController.advance": {
                    "total": 583.8394446999009,
                    "count": 22182,
                    "self": 0.6040214999229647,
                    "children": {
                        "env_step": {
                            "total": 366.50718039963976,
                            "count": 22182,
                            "self": 314.6305896996637,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 51.55080489793909,
                                    "count": 22182,
                                    "self": 2.32627019751817,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 49.22453470042092,
                                            "count": 39034,
                                            "self": 49.22453470042092
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.3257858020369895,
                                    "count": 22181,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 523.0899376003072,
                                            "count": 22181,
                                            "is_parallel": true,
                                            "self": 310.5949654003198,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.003385200019693002,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.0007755000260658562,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.002609699993627146,
                                                            "count": 4,
                                                            "is_parallel": true,
                                                            "self": 0.002609699993627146
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 212.4915869999677,
                                                    "count": 22181,
                                                    "is_parallel": true,
                                                    "self": 3.6641000936797354,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 6.559313899226254,
                                                            "count": 22181,
                                                            "is_parallel": true,
                                                            "self": 6.559313899226254
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 191.72060090312152,
                                                            "count": 22181,
                                                            "is_parallel": true,
                                                            "self": 191.72060090312152
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 10.547572103940183,
                                                            "count": 44362,
                                                            "is_parallel": true,
                                                            "self": 5.198161705106031,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 5.349410398834152,
                                                                    "count": 88724,
                                                                    "is_parallel": true,
                                                                    "self": 5.349410398834152
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 216.7282428003382,
                            "count": 44362,
                            "self": 1.2293734002159908,
                            "children": {
                                "process_trajectory": {
                                    "total": 36.32286720033153,
                                    "count": 44362,
                                    "self": 36.32286720033153
                                },
                                "_update_policy": {
                                    "total": 179.17600219979067,
                                    "count": 238,
                                    "self": 64.80509169859579,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 114.37091050119489,
                                            "count": 23460,
                                            "self": 114.37091050119489
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "TrainerController._save_models": {
                    "total": 0.306622400006745,
                    "count": 1,
                    "self": 0.03413179999915883,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.2724906000075862,
                            "count": 2,
                            "self": 0.2724906000075862
                        }
                    }
                }
            }
        }
    }
}